{"id":"StockScreenClaude-08c","title":"T2: Use case tests with fake repositories (no DB/network)","description":"## What\nWrite unit tests for use cases using fake (in-memory) repositories and providers. These tests validate workflow logic without any infrastructure dependencies.\n\n## Files to Create\n- backend/tests/unit/use_cases/conftest.py — Shared fake implementations:\n  - FakeScanRepository (in-memory dict)\n  - FakeScanResultRepository (in-memory list)\n  - FakeUniverseRepository (returns configured symbol lists)\n  - FakeStockDataProvider (returns deterministic StockData)\n  - FakeProgressSink (collects events in a list)\n  - FakeCancellationToken (configurable)\n\n- backend/tests/unit/use_cases/test_create_scan.py\n  - Happy path: creates scan with expected fields\n  - Idempotency: same key returns existing scan\n  - Universe resolution called correctly\n\n- backend/tests/unit/use_cases/test_run_bulk_scan.py\n  - Happy path: processes all symbols, writes results\n  - Cancellation: stops between chunks, status updated\n  - Checkpointing: resume skips already-processed symbols\n  - Error handling: failed symbols don't crash the run\n\n- backend/tests/unit/use_cases/test_get_scan_results.py\n  - Pagination: correct page size and total count\n  - Filtering: FilterSpec applied correctly\n  - Sorting: SortSpec applied correctly\n\n## Why Fake Repos\nFake repos (not mocks) test actual behavior. A FakeScanRepository stores data in a dict and returns it — so we verify the use case calls the right methods with the right data, not just that it 'called save once'.\n\n## Acceptance Criteria\n- All use case happy paths tested\n- Error and edge cases covered\n- Tests run in \u003c2 seconds total (no IO)\n- No mocking of domain logic — only infrastructure ports are faked\n\n## Depends On\n- C1 (RunBulkScanUseCase must exist)","status":"open","priority":2,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-17T00:05:47.289944+08:00","created_by":"David Ten","updated_at":"2026-02-17T00:05:47.289944+08:00","dependencies":[{"issue_id":"StockScreenClaude-08c","depends_on_id":"StockScreenClaude-5pd","type":"blocks","created_at":"2026-02-17T00:08:12.07183+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-0ch","title":"F1: Create BuildDailyFeatureSnapshotUseCase","description":"## What\nImplement the core offline pipeline use case that builds a complete feature snapshot for a trading day.\n\n## Files to Create\n- backend/app/use_cases/feature_store/build_daily_snapshot.py\n\n## Algorithm (from v3 plan Phase F)\n1. Determine as_of_date trading day (domain time policy — skip weekends/holidays)\n2. FeatureRunRepository.start_run(...) with correlation_id\n3. Resolve universe of active symbols via UniverseRepository (likely 'ALL' or configured set)\n4. Persist resolved universe: FeatureStoreRepository.save_run_universe_symbols(run_id, symbols)\n5. Merge data requirements from all configured screeners\n6. Call StockDataProvider.prepare_bulk(symbols, requirements, as_of_date, allow_partial=True)\n7. For each symbol with data:\n   - Run orchestrator (domain, pure) with pre-fetched data\n   - Collect ScanResultItemDomain\n8. FeatureStoreRepository.upsert_snapshot_rows(run_id, rows)\n9. Run DQ checks (domain/feature_store/quality.py):\n   - Row count vs expected universe size\n   - Null rate on composite_score\n   - Score distribution sanity\n10. If DQ fails → mark_quarantined(run_id, dq_report) — keep data for inspection\n11. If DQ passes → publish_atomically(run_id) — atomic pointer update\n12. Emit completion ProgressEvent with stats\n\n## Error Handling\n- allow_partial=True means partial upstream failures don't crash the whole run\n- Failed symbols logged in warnings_json\n- Run is still publishable if DQ passes despite partial failures (configurable threshold)\n\n## Acceptance Criteria\n- Can build and publish a snapshot for a given date\n- Quarantines on DQ failure (doesn't publish bad data)\n- Handles partial upstream failures gracefully\n- Idempotent: re-running for same date creates a new run (doesn't corrupt existing)\n- Unit-testable with fake repos and fake StockDataProvider\n\n## Depends On\n- E3 (Feature Store repositories)\n- B3 (port-based orchestrator)\n- C1 (pattern for use case structure)","status":"open","priority":2,"issue_type":"feature","owner":"davidten7@gmail.com","created_at":"2026-02-17T00:04:08.218565+08:00","created_by":"David Ten","updated_at":"2026-02-17T00:04:08.218565+08:00","dependencies":[{"issue_id":"StockScreenClaude-0ch","depends_on_id":"StockScreenClaude-rwf","type":"blocks","created_at":"2026-02-17T00:08:10.294006+08:00","created_by":"David Ten"},{"issue_id":"StockScreenClaude-0ch","depends_on_id":"StockScreenClaude-7yl","type":"blocks","created_at":"2026-02-17T00:08:10.598267+08:00","created_by":"David Ten"},{"issue_id":"StockScreenClaude-0ch","depends_on_id":"StockScreenClaude-5pd","type":"blocks","created_at":"2026-02-17T00:08:10.987087+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-0rg","title":"Add Cache Health API Endpoint","description":"Add GET /v1/cache/health endpoint that exposes the cache health status. Returns status, SPY last date, expected date, task progress, and warmup metadata.","status":"closed","priority":1,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-05T15:01:05.109469+08:00","created_by":"David Ten","updated_at":"2026-02-05T15:10:12.116751+08:00","closed_at":"2026-02-05T15:10:12.116751+08:00","close_reason":"Added GET /v1/cache/health, POST /v1/cache/refresh, and POST /v1/cache/force-cancel endpoints","dependencies":[{"issue_id":"StockScreenClaude-0rg","depends_on_id":"StockScreenClaude-szs","type":"blocks","created_at":"2026-02-05T15:02:50.848677+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-17g","title":"T14: Add universe selector UI with exchange/index options","status":"closed","priority":3,"issue_type":"feature","owner":"davidten7@gmail.com","created_at":"2026-02-16T14:49:35.1814+08:00","created_by":"David Ten","updated_at":"2026-02-16T14:58:24.174723+08:00","closed_at":"2026-02-16T14:58:24.174723+08:00","close_reason":"Implemented in universe unification session","dependencies":[{"issue_id":"StockScreenClaude-17g","depends_on_id":"StockScreenClaude-bkd","type":"blocks","created_at":"2026-02-16T14:50:39.710704+08:00","created_by":"David Ten"},{"issue_id":"StockScreenClaude-17g","depends_on_id":"StockScreenClaude-hq1","type":"blocks","created_at":"2026-02-16T14:50:40.025699+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-1wy","title":"Z2: Consolidate Pydantic schemas after migration","description":"## What\nDuring the strangler migration, some Pydantic schemas may have been duplicated (inline in router vs separate files). Consolidate them into a clean schema layer.\n\n## Current State\nThe scan router (backend/app/api/v1/scans.py) has many schemas defined inline (~100+ lines of Pydantic models at the top of the file). During the refactor, domain types were added alongside these.\n\n## Target State\n- backend/app/schemas/scanning.py — All scan-related request/response schemas\n- backend/app/schemas/feature_store.py — Feature store API schemas\n- Remove inline schema definitions from routers\n- Clear mapping: domain types ↔ API schemas (explicit conversion functions)\n\n## Consolidation Rules\n- Request schemas: Pydantic models that validate API input → kept in schemas/\n- Response schemas: Pydantic models that format API output → kept in schemas/\n- Domain types: dataclasses in domain/ — NOT Pydantic (no validation dependencies)\n- Conversion: explicit to_domain() and from_domain() methods on schemas\n\n## Acceptance Criteria\n- No inline Pydantic models in router files\n- Clear schema files organized by feature area\n- Domain types remain pure (no Pydantic dependency)\n- All API contracts unchanged (same JSON shape)\n- OpenAPI docs still generate correctly\n\n## Depends On\n- Z1 (legacy paths removed, so we know what's actually used)","status":"open","priority":3,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-17T00:06:40.283661+08:00","created_by":"David Ten","updated_at":"2026-02-17T00:06:40.283661+08:00","dependencies":[{"issue_id":"StockScreenClaude-1wy","depends_on_id":"StockScreenClaude-jhs","type":"blocks","created_at":"2026-02-17T00:08:14.070113+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-26z","title":"T7: Create idempotent schema migration and backfill script","status":"closed","priority":1,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-16T14:49:09.775217+08:00","created_by":"David Ten","updated_at":"2026-02-16T14:58:23.858771+08:00","closed_at":"2026-02-16T14:58:23.858771+08:00","close_reason":"Implemented in universe unification session","dependencies":[{"issue_id":"StockScreenClaude-26z","depends_on_id":"StockScreenClaude-47f","type":"blocks","created_at":"2026-02-16T14:50:11.223107+08:00","created_by":"David Ten"},{"issue_id":"StockScreenClaude-26z","depends_on_id":"StockScreenClaude-eeh","type":"blocks","created_at":"2026-02-16T14:50:11.507008+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-2sm","title":"EPIC-Z: Cleanup — Remove Legacy Path, Consolidate Schemas","description":"Final cleanup after the feature store is stable and all endpoints are migrated. Remove the legacy compute-per-request scan path (or downgrade to opt-in debug mode). Consolidate Pydantic schemas that were duplicated during the strangler migration. Corresponds to Phase PR9 of the v3 architecture plan.","status":"open","priority":3,"issue_type":"epic","owner":"davidten7@gmail.com","created_at":"2026-02-16T23:59:08.865032+08:00","created_by":"David Ten","updated_at":"2026-02-16T23:59:08.865032+08:00"}
{"id":"StockScreenClaude-2u8","title":"T4: Write unit tests for UniverseResolver","status":"closed","priority":2,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-16T14:49:00.726511+08:00","created_by":"David Ten","updated_at":"2026-02-16T14:58:23.715763+08:00","closed_at":"2026-02-16T14:58:23.715763+08:00","close_reason":"Implemented in universe unification session","dependencies":[{"issue_id":"StockScreenClaude-2u8","depends_on_id":"StockScreenClaude-z91","type":"blocks","created_at":"2026-02-16T14:49:59.995548+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-3e8","title":"T4: Golden scan parity test — legacy vs feature store comparison","description":"## What\nCreate a golden test that verifies feature store results match legacy scan results for a fixed set of tickers. This is the ultimate validation that the refactor preserved behavior.\n\n## Files to Create\n- backend/tests/parity/test_scan_parity.py\n- backend/tests/parity/fixtures/ — Recorded test data\n\n## Test Design\n1. Pick 20 stable large-cap tickers (AAPL, MSFT, GOOGL, etc.)\n2. Record legacy scan results for these tickers at a fixed as_of_date\n3. Build a feature store snapshot for the same date with same config\n4. Compare field by field:\n   - composite_score: within 0.01 tolerance (floating point)\n   - overall_rating: exact match\n   - passes_count: exact match\n   - Per-screener scores: within 0.01 tolerance\n5. Compare filtered/sorted result ordering for common filter sets\n\n## Why Tolerance\nFloating point arithmetic may differ slightly between compute-per-request and batch paths due to ordering of operations. 0.01 tolerance prevents false failures while catching real regressions.\n\n## Acceptance Criteria\n- 20-ticker parity test passes\n- Composite scores match within tolerance\n- Ratings match exactly\n- Filter/sort ordering stable\n- Test is reproducible (uses recorded fixture data, not live API)\n\n## Depends On\n- G2 (dual-source query must work to compare both paths)","status":"open","priority":2,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-17T00:06:02.69785+08:00","created_by":"David Ten","updated_at":"2026-02-17T00:06:02.69785+08:00","dependencies":[{"issue_id":"StockScreenClaude-3e8","depends_on_id":"StockScreenClaude-78r","type":"blocks","created_at":"2026-02-17T00:08:12.652449+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-3hc","title":"T11: Update scan response schemas to include structured universe fields","status":"closed","priority":2,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-16T14:49:22.357834+08:00","created_by":"David Ten","updated_at":"2026-02-16T14:58:24.042519+08:00","closed_at":"2026-02-16T14:58:24.042519+08:00","close_reason":"Implemented in universe unification session","dependencies":[{"issue_id":"StockScreenClaude-3hc","depends_on_id":"StockScreenClaude-47f","type":"blocks","created_at":"2026-02-16T14:50:27.785739+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-3wq","title":"T15: Update scan history label formatting","status":"closed","priority":3,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-16T14:49:38.938522+08:00","created_by":"David Ten","updated_at":"2026-02-16T14:58:24.220295+08:00","closed_at":"2026-02-16T14:58:24.220295+08:00","close_reason":"Implemented in universe unification session","dependencies":[{"issue_id":"StockScreenClaude-3wq","depends_on_id":"StockScreenClaude-3hc","type":"blocks","created_at":"2026-02-16T14:50:43.0457+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-3z0","title":"B1: Extract composite score + rating policies to domain layer","description":"## What\nExtract pure scoring and rating logic from ScanOrchestrator into domain modules. These become pure functions with zero IO dependencies.\n\n## Current State\n- backend/app/scanners/scan_orchestrator.py contains:\n  - _calculate_composite_score() (~line 350-400): weighted_average, maximum, minimum methods\n  - _calculate_overall_rating() (~line 400-450): threshold-based rating from composite score\n\n## Files to Create\n- backend/app/domain/scanning/composite.py — CompositeScorer class\n  - Methods: weighted_average, maximum, minimum\n  - Input: dict of screener scores + weights\n  - Output: float composite score\n  - Pure function, no IO\n\n- backend/app/domain/scanning/rating.py — RatingCalculator class\n  - Input: composite score, pass rates, thresholds config\n  - Output: integer rating (1-5 scale)\n  - Must match EXACT current thresholds from scan_orchestrator.py\n  - Pure function, no IO\n\n## Why This Matters\nThese are the core business rules of the screener. By extracting them to domain/, they become:\n1. Unit-testable without mocking anything\n2. Reusable by both online scan and offline feature store\n3. Protected from accidental IO coupling\n\n## Acceptance Criteria\n- CompositeScorer produces identical results to current _calculate_composite_score\n- RatingCalculator produces identical results to current _calculate_overall_rating\n- Both have zero imports outside domain/ and stdlib\n- Boundary test (from A1) passes\n- Existing scan results unchanged (integration verify)\n\n## Depends On\n- A1 (domain/scanning/ package must exist)","status":"open","priority":1,"issue_type":"feature","owner":"davidten7@gmail.com","created_at":"2026-02-17T00:00:12.299098+08:00","created_by":"David Ten","updated_at":"2026-02-17T00:00:12.299098+08:00","dependencies":[{"issue_id":"StockScreenClaude-3z0","depends_on_id":"StockScreenClaude-7tx","type":"blocks","created_at":"2026-02-17T00:08:09.981428+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-47f","title":"T5: Add structured universe columns to Scan model","status":"closed","priority":1,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-16T14:49:03.557839+08:00","created_by":"David Ten","updated_at":"2026-02-16T14:58:23.771622+08:00","closed_at":"2026-02-16T14:58:23.771622+08:00","close_reason":"Implemented in universe unification session","dependencies":[{"issue_id":"StockScreenClaude-47f","depends_on_id":"StockScreenClaude-eeh","type":"blocks","created_at":"2026-02-16T14:50:03.053308+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-4g6","title":"C2: Rewrite Celery run_bulk_scan task as thin wrapper","description":"## What\nRewrite the Celery task to be a thin wrapper that calls RunBulkScanUseCase.\n\n## Current State (backend/app/tasks/scan_tasks.py)\n~1120 lines of mixed Celery + business logic.\n\n## Target State\nThe task becomes ~30-50 lines:\n1. Open UoW (session)\n2. Create CancellationToken (checks Celery revoke state)\n3. Create ProgressSink (updates Celery task state for polling)\n4. Call RunBulkScanUseCase.execute(scan_id, correlation_id, cancellation_token)\n5. Return result summary\n\n## Files to Modify\n- backend/app/tasks/scan_tasks.py — Replace run_bulk_scan implementation\n\n## Files to Create\n- backend/app/interfaces/tasks/scan_tasks.py — New thin task (eventually replaces old one)\n- backend/app/infra/celery_progress_sink.py — ProgressSink adapter that updates Celery task meta\n- backend/app/infra/celery_cancellation_token.py — CancellationToken that checks task.is_revoked()\n\n## Migration Strategy\nUse strangler pattern: new task calls use case, old code stays until verified.\nCan use a feature flag (env var) to switch between old and new path during testing.\n\n## Acceptance Criteria\n- Task file contains NO business rules (only wiring + Celery state management)\n- Task is safe to retry without duplicating results\n- Progress polling still works for frontend\n- All existing scan functionality preserved\n\n## Depends On\n- C1 (RunBulkScanUseCase must exist)","status":"open","priority":1,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-17T00:02:19.171587+08:00","created_by":"David Ten","updated_at":"2026-02-17T00:02:19.171587+08:00","dependencies":[{"issue_id":"StockScreenClaude-4g6","depends_on_id":"StockScreenClaude-5pd","type":"blocks","created_at":"2026-02-17T00:08:10.65459+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-5pd","title":"C1: Create RunBulkScanUseCase with progress, cancellation, checkpointing","description":"## What\nCreate the use case that contains all scan execution business logic, extracted from the current Celery task file.\n\n## Current State (backend/app/tasks/scan_tasks.py, ~1120 lines)\nThe run_bulk_scan Celery task currently handles:\n- Loading Scan record from DB\n- Resolving universe symbols\n- Updating scan status/progress in DB\n- Running batch orchestration (chunked)\n- Persisting scan results\n- Cleanup and retention policies\nAll mixed with Celery-specific state updates and error handling.\n\n## Files to Create\n- backend/app/use_cases/scanning/run_bulk_scan.py — RunBulkScanUseCase class\n\n## RunBulkScanUseCase.execute(scan_id, correlation_id, cancellation_token) should:\n1. Load Scan record via ScanRepository port\n2. Resolve universe symbols via UniverseRepository port\n3. Update scan status to 'running' via ScanRepository\n4. Emit ProgressEvents via ProgressSink port (append-only timeline)\n5. Process symbols in chunks:\n   a. Check cancellation_token between chunks\n   b. Save checkpoint after each chunk (for resumability)\n   c. Call orchestrator (domain) with pre-fetched data\n6. Persist results via ScanResultRepository port\n7. Update scan status to 'completed' with final stats\n8. Handle errors: update status to 'failed' with error details\n\n## Ports Required (add to domain/scanning/ports.py)\n- ScanRepository: create, get, update_status\n- ScanResultRepository: bulk_upsert, query_results\n- UniverseRepository: resolve_symbols\n- ProgressSink: emit(ProgressEvent)\n- CancellationToken: is_cancelled() -\u003e bool\n\n## Key Design Principle\nThe use case orchestrates the WORKFLOW but contains no IO code. All IO goes through ports. This means:\n- Unit-testable with fake repos (no DB, no Redis, no Celery)\n- Retry-safe: checkpointing ensures no duplicate work on resume\n- Celery-agnostic: could be called from CLI, API, or test\n\n## Acceptance Criteria\n- Use case handles full scan lifecycle (create → running → completed/failed)\n- Progress events emitted at meaningful milestones\n- Cancellation checked between chunks\n- Checkpoint save/restore prevents duplicate processing on retry\n- Zero Celery imports in this file\n- Unit test with fake repos covers happy path + cancellation + resume\n\n## Depends On\n- B3 (port-based orchestrator)\n- A3 (CreateScanUseCase pattern established)","status":"open","priority":1,"issue_type":"feature","owner":"davidten7@gmail.com","created_at":"2026-02-17T00:02:11.007226+08:00","created_by":"David Ten","updated_at":"2026-02-17T00:02:11.007226+08:00","dependencies":[{"issue_id":"StockScreenClaude-5pd","depends_on_id":"StockScreenClaude-7yl","type":"blocks","created_at":"2026-02-17T00:08:09.985096+08:00","created_by":"David Ten"},{"issue_id":"StockScreenClaude-5pd","depends_on_id":"StockScreenClaude-ban","type":"blocks","created_at":"2026-02-17T00:08:10.334731+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-5pe","title":"T12: Update retention cleanup to use universe_key","status":"closed","priority":1,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-16T14:49:25.381418+08:00","created_by":"David Ten","updated_at":"2026-02-16T14:58:24.08469+08:00","closed_at":"2026-02-16T14:58:24.08469+08:00","close_reason":"Implemented in universe unification session","dependencies":[{"issue_id":"StockScreenClaude-5pe","depends_on_id":"StockScreenClaude-47f","type":"blocks","created_at":"2026-02-16T14:50:32.705718+08:00","created_by":"David Ten"},{"issue_id":"StockScreenClaude-5pe","depends_on_id":"StockScreenClaude-bkd","type":"blocks","created_at":"2026-02-16T14:50:33.02816+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-5rl","title":"Add Smart Refresh Celery Task","description":"Add smart_refresh_cache(mode) task with SPY-first fetching and market cap prioritization. Supports auto (cached symbols) and full (entire universe) modes. Includes heartbeat updates for stuck detection.","status":"closed","priority":1,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-05T15:01:07.274244+08:00","created_by":"David Ten","updated_at":"2026-02-05T15:09:28.201251+08:00","closed_at":"2026-02-05T15:09:28.201251+08:00","close_reason":"Added smart_refresh_cache(mode) task with SPY-first fetching, market cap prioritization, and heartbeat updates"}
{"id":"StockScreenClaude-65k","title":"T5: Provider contract tests — retry, partial failure, rate limits","description":"## What\nWrite contract tests for the StockDataProvider adapter to verify it handles upstream failures correctly without flakiness.\n\n## Files to Create\n- backend/tests/unit/infra/test_stock_data_provider.py\n\n## Test Cases\n- Retry with backoff: provider retries transient failures with jittered backoff\n- allow_partial=True: returns data for successful symbols, marks failures (doesn't crash)\n- allow_partial=False: raises on any failure\n- Rate limit enforcement: verify throttle point limits request rate\n- Timeout handling: long-running requests don't block forever\n- Missing data: structured 'missing inputs' returned, not exceptions\n\n## Test Strategy\nUse stub/recorded responses (not live API calls) to ensure:\n- Tests are fast and deterministic\n- No rate limit consumption\n- Specific failure modes can be triggered reliably\n\n## Acceptance Criteria\n- All failure modes tested without hitting real APIs\n- Rate limit enforcement verified (timing-based or counter-based)\n- Partial failure semantics verified\n- Tests run in \u003c5 seconds\n\n## Depends On\n- B3 (StockDataProvider port and adapter must exist)","status":"open","priority":3,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-17T00:06:09.361318+08:00","created_by":"David Ten","updated_at":"2026-02-17T00:06:09.361318+08:00","dependencies":[{"issue_id":"StockScreenClaude-65k","depends_on_id":"StockScreenClaude-7yl","type":"blocks","created_at":"2026-02-17T00:08:12.934603+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-6dx","title":"G4: Add feature-store-native endpoints (runs list, explain, compare)","description":"## What\nAdd new API endpoints that expose Feature Store capabilities not available in the legacy scan API. These are compelling features that justify the refactor.\n\n## Endpoints to Create\n\n### GET /api/v1/features/runs\n- List all feature runs with status, DQ summary, publish state\n- Useful for monitoring and admin dashboards\n- Shows: as_of_date, status, row_count, publish time, DQ warnings\n\n### GET /api/v1/scans/{scan_id}/explain/{symbol}\n- Detailed pass/fail breakdown for a specific stock\n- Per-screener criterion results\n- Composite score contribution from each screener\n- Rating threshold explanation\n- Uses ExplainStockUseCase (D6)\n\n### GET /api/v1/features/compare?run_a=...\u0026run_b=...\n- Compare two feature runs to find movers\n- Returns: symbols with biggest score/rating changes\n- Useful for tracking day-over-day changes\n\n## Files to Create\n- backend/app/use_cases/feature_store/compare_runs.py — CompareFeatureRunsUseCase\n- backend/app/interfaces/api/v1/features.py — New router for feature store endpoints\n\n## Files to Modify\n- backend/app/api/v1/scans.py — Add explain endpoint\n- backend/app/main.py — Register new features router\n\n## Acceptance Criteria\n- All three endpoints working with proper error handling\n- Runs list shows accurate status and DQ info\n- Compare shows meaningful movers (score deltas, rating changes)\n- Explain provides actionable per-criterion breakdown\n\n## Depends On\n- G2 (dual-source query established)","status":"open","priority":3,"issue_type":"feature","owner":"davidten7@gmail.com","created_at":"2026-02-17T00:05:31.174187+08:00","created_by":"David Ten","updated_at":"2026-02-17T00:05:31.174187+08:00","dependencies":[{"issue_id":"StockScreenClaude-6dx","depends_on_id":"StockScreenClaude-78r","type":"blocks","created_at":"2026-02-17T00:08:11.510179+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-6i8","title":"Refactor CacheStatus Component","description":"Replace 3 price elements with single Cache chip. Handle 6 states with appropriate colors/icons. Add dropdown for Full Refresh. Implement dynamic polling and completion notifications.","status":"closed","priority":1,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-05T15:01:13.213356+08:00","created_by":"David Ten","updated_at":"2026-02-05T15:11:54.009022+08:00","closed_at":"2026-02-05T15:11:54.009022+08:00","close_reason":"Refactored CacheStatus to use unified cache health endpoint with 6 states, dropdown menu, dynamic polling, and completion notifications","dependencies":[{"issue_id":"StockScreenClaude-6i8","depends_on_id":"StockScreenClaude-0rg","type":"blocks","created_at":"2026-02-05T15:02:51.540535+08:00","created_by":"David Ten"},{"issue_id":"StockScreenClaude-6i8","depends_on_id":"StockScreenClaude-k92","type":"blocks","created_at":"2026-02-05T15:02:51.757012+08:00","created_by":"David Ten"},{"issue_id":"StockScreenClaude-6i8","depends_on_id":"StockScreenClaude-c6b","type":"blocks","created_at":"2026-02-05T15:02:51.96787+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-6of","title":"EPIC-G: Feature Store Switch — Bind Scans, Dual-Source Query, New Endpoints","description":"Switch scan results endpoints to read from the Feature Store instead of computing per-request. Adds Scan.feature_run_id binding, dual-source query logic (feature store when bound, legacy fallback), migrates ancillary endpoints, and adds new feature-store-native endpoints (runs list, explain, compare). Corresponds to Phase G of the v3 architecture plan.","status":"open","priority":2,"issue_type":"epic","owner":"davidten7@gmail.com","created_at":"2026-02-16T23:59:08.757528+08:00","created_by":"David Ten","updated_at":"2026-02-16T23:59:08.757528+08:00"}
{"id":"StockScreenClaude-74o","title":"B2: Define domain types — ScanConfig, ScreenerOutput, ScanResultItem","description":"## What\nCreate domain-layer data types that represent scanning concepts independent of DB schema or API format.\n\n## Files to Create\n- backend/app/domain/scanning/models.py containing:\n\n  @dataclass ScanConfig:\n    screeners: list[ScreenerName]\n    composite_method: CompositeMethod  # enum: weighted_average, maximum, minimum\n    criteria: dict  # screener-specific criteria\n    universe: UniverseDefinition\n    weights: dict[ScreenerName, float] | None\n\n  @dataclass ScreenerOutput:\n    screener: ScreenerName\n    score: float\n    passes: bool\n    rating: int\n    breakdown: dict  # per-criterion pass/fail details\n    details: dict  # additional data (e.g., RS rating value)\n\n  @dataclass ScanResultItemDomain:\n    symbol: str\n    composite_score: float\n    overall_rating: int\n    screener_outputs: dict[ScreenerName, ScreenerOutput]\n    passes_count: int\n    total_screeners: int\n\n  class ScreenerName(str, Enum): minervini, canslim, ipo, volume_breakthrough, custom\n\n  class CompositeMethod(str, Enum): weighted_average, maximum, minimum\n\n- backend/app/domain/scanning/filter_spec.py:\n  @dataclass FilterSpec — pure filter representation (field, op, value)\n  @dataclass SortSpec — (field, direction)\n  @dataclass PagingSpec — (offset, limit)\n\n## Why\nDomain types decouple business logic from SQLAlchemy models and Pydantic schemas. The same ScanResultItemDomain can be produced by online scan OR feature store query — the consumer doesn't care about the source.\n\n## Acceptance Criteria\n- All types are pure dataclasses/enums with no IO imports\n- Types pass boundary test (domain-only imports)\n- Types can represent all current scan result fields\n\n## Depends On\n- A2 (for integration with wiring), A1 implicitly","status":"open","priority":1,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-17T00:01:33.159963+08:00","created_by":"David Ten","updated_at":"2026-02-17T00:01:33.159963+08:00","dependencies":[{"issue_id":"StockScreenClaude-74o","depends_on_id":"StockScreenClaude-7lw","type":"blocks","created_at":"2026-02-17T00:08:10.659086+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-78r","title":"G2: Update GetScanResultsUseCase for dual-source query (feature store + legacy)","description":"## What\nUpdate GetScanResultsUseCase to read from Feature Store when the scan is bound to a feature run, falling back to legacy scan_results for unbound scans.\n\n## Files to Modify\n- backend/app/use_cases/scanning/get_scan_results.py — Add dual-source logic:\n\n  def execute(scan_id, filter_spec, sort_spec, paging_spec):\n    scan = scan_repo.get(scan_id)\n    if scan.feature_run_id:\n      # Query feature store\n      return feature_store_repo.query_run(scan.feature_run_id, filter_spec, sort_spec, paging_spec)\n    else:\n      # Legacy path: query scan_results table\n      return scan_result_repo.query_results(scan_id, filter_spec, sort_spec, paging_spec)\n\n## Files to Create/Modify\n- backend/app/infra/query/feature_store_query_sqlalchemy.py — Query builder for stock_feature_daily table\n  - Must support same FilterSpec fields as legacy query builder\n  - JSON extraction for details_json fields\n\n## Key Consideration\nBoth paths must return the same domain type (Page[ScanResultItemDomain]) so the router doesn't know or care about the source. This is the Liskov Substitution Principle in action.\n\n## Acceptance Criteria\n- Bound scans query feature store\n- Unbound scans query legacy scan_results\n- Same response format regardless of source\n- No breaking API changes\n- Performance: feature store queries should be faster (pre-indexed, no JSON parsing in hot path)\n\n## Depends On\n- G1 (Scan.feature_run_id must exist)\n- D1 (GetScanResultsUseCase must exist)\n- E3 (FeatureStoreRepository must exist)","status":"open","priority":2,"issue_type":"feature","owner":"davidten7@gmail.com","created_at":"2026-02-17T00:04:41.289254+08:00","created_by":"David Ten","updated_at":"2026-02-17T00:04:41.289254+08:00","dependencies":[{"issue_id":"StockScreenClaude-78r","depends_on_id":"StockScreenClaude-ile","type":"blocks","created_at":"2026-02-17T00:08:10.328863+08:00","created_by":"David Ten"},{"issue_id":"StockScreenClaude-78r","depends_on_id":"StockScreenClaude-iai","type":"blocks","created_at":"2026-02-17T00:08:10.65599+08:00","created_by":"David Ten"},{"issue_id":"StockScreenClaude-78r","depends_on_id":"StockScreenClaude-rwf","type":"blocks","created_at":"2026-02-17T00:08:10.945673+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-7lw","title":"A2: Add wiring/bootstrap.py dependency injection graph","description":"## What\nCreate the dependency injection bootstrap module that wires together repositories, providers, and use cases. This is the single place where concrete implementations are bound to abstract ports.\n\n## Files to Create\n- backend/app/wiring/bootstrap.py\n\n## Implementation Details\nbootstrap.py should provide:\n- get_uow() -\u003e UnitOfWork factory (wraps SessionLocal)\n- get_scan_repo() -\u003e ScanRepository (SQLAlchemy impl)\n- get_scan_result_repo() -\u003e ScanResultRepository\n- get_universe_repo() -\u003e UniverseRepository\n- get_stock_data_provider() -\u003e StockDataProvider (DataPreparationLayer adapter)\n- get_create_scan_use_case() -\u003e CreateScanUseCase (wired with repos)\n\nUse FastAPI's Depends() integration pattern so routers can inject use cases.\n\n## Also Create\n- backend/app/infra/db/uow.py — UnitOfWork class wrapping SQLAlchemy session lifecycle\n\n## Why\nCentralizing wiring prevents each module from importing concrete implementations directly. This is what makes the port/adapter pattern actually work — you swap implementations in ONE place.\n\n## Acceptance Criteria\n- bootstrap.py can instantiate all registered use cases\n- UnitOfWork properly manages session lifecycle (commit/rollback)\n- FastAPI Depends() integration works for router injection\n- No changes to existing behavior\n\n## Depends On\n- A1 (package structure must exist)","status":"closed","priority":1,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-16T23:59:53.417027+08:00","created_by":"David Ten","updated_at":"2026-02-17T06:33:19.067016+08:00","closed_at":"2026-02-17T06:33:19.067016+08:00","close_reason":"Bootstrap module, UoW, ports, 3 repos, 1 provider adapter all created. Boundary tests pass. FastAPI Depends() pattern verified.","dependencies":[{"issue_id":"StockScreenClaude-7lw","depends_on_id":"StockScreenClaude-7tx","type":"blocks","created_at":"2026-02-17T00:08:09.676831+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-7t6","title":"D2: Create GetFilterOptionsUseCase","description":"## What\nExtract filter options endpoint logic into a use case.\n\n## Current State\nThe filter-options endpoint in scans.py queries distinct values from scan_results for dropdown population (sectors, industries, screener names, etc.).\n\n## Files to Create\n- backend/app/use_cases/scanning/get_filter_options.py — GetFilterOptionsUseCase\n  - Input: scan_id\n  - Output: FilterOptions dataclass (sectors, industries, exchanges, etc.)\n  - Uses ScanResultRepository port\n\n## Acceptance Criteria\n- Returns same filter option values as current endpoint\n- Router is thin wrapper\n- Works with both legacy scan_results and (future) feature store\n\n## Depends On\n- D1 (establishes query patterns and ScanResultRepository)","status":"open","priority":2,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-17T00:02:34.050336+08:00","created_by":"David Ten","updated_at":"2026-02-17T00:02:34.050336+08:00","dependencies":[{"issue_id":"StockScreenClaude-7t6","depends_on_id":"StockScreenClaude-iai","type":"blocks","created_at":"2026-02-17T00:08:09.763661+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-7tb","title":"D6: Create ExplainStockUseCase (new feature — pass/fail breakdown)","description":"## What\nNEW FEATURE: Create a use case that explains why a stock passed or failed each screener, with per-criterion breakdown.\n\n## Design\n- backend/app/use_cases/scanning/explain_stock.py — ExplainStockUseCase\n  - Input: scan_id, symbol\n  - Output: StockExplanation dataclass containing:\n    - Per-screener pass/fail with individual criterion results\n    - Composite score breakdown (which screeners contributed what)\n    - Rating explanation (threshold that was met/missed)\n    - Key metrics that drove the result\n\n## Domain Types to Add\n- backend/app/domain/scanning/models.py — Add StockExplanation, CriterionResult dataclasses\n\n## Data Source\n- Legacy mode: reads from ScanResult.details JSON (already contains per-screener breakdown)\n- Feature store mode (future): reads from StockFeatureDaily.details_json\n\n## Why This Feature\nUsers currently see a composite score but can't understand WHY a stock scored a certain way. This 'explain' feature builds trust and helps users learn the screening methodology.\n\n## Acceptance Criteria\n- Returns per-screener pass/fail with criteria details\n- Works with existing scan results (legacy mode)\n- Designed to also work with feature store (future G-series)\n- Has unit test with fake repo\n\n## Depends On\n- D1 (establishes use case and repository patterns)","status":"open","priority":2,"issue_type":"feature","owner":"davidten7@gmail.com","created_at":"2026-02-17T00:02:52.912574+08:00","created_by":"David Ten","updated_at":"2026-02-17T00:02:52.912574+08:00","dependencies":[{"issue_id":"StockScreenClaude-7tb","depends_on_id":"StockScreenClaude-iai","type":"blocks","created_at":"2026-02-17T00:08:11.278262+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-7tx","title":"A1: Create layered architecture package structure + boundary checks","description":"ROOT TASK — no dependencies, start here.\n\n## What\nCreate the new package structure for the layered architecture and add enforceable import boundary checks.\n\n## Files to Create\n- backend/app/domain/__init__.py\n- backend/app/domain/common/__init__.py, errors.py, types.py\n- backend/app/domain/scanning/__init__.py\n- backend/app/domain/feature_store/__init__.py\n- backend/app/use_cases/__init__.py\n- backend/app/use_cases/scanning/__init__.py\n- backend/app/use_cases/feature_store/__init__.py\n- backend/app/infra/__init__.py\n- backend/app/infra/db/__init__.py\n- backend/app/infra/db/repositories/__init__.py\n- backend/app/infra/cache/__init__.py\n- backend/app/infra/providers/__init__.py\n- backend/app/infra/query/__init__.py\n- backend/app/interfaces/__init__.py\n- backend/app/interfaces/api/__init__.py\n- backend/app/interfaces/tasks/__init__.py\n- backend/app/wiring/__init__.py\n\n## Boundary Enforcement\nAdd tests/unit/test_layer_boundaries.py that checks:\n- domain/ imports ONLY domain/ and stdlib\n- use_cases/ imports domain/ + stdlib only (no infra/interfaces)\n- Fail on violation so CI catches drift\n\n## Acceptance Criteria\n- All packages importable with no errors\n- Boundary test passes (green)\n- No behavior change to existing application\n- Existing tests still pass","status":"closed","priority":1,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-16T23:59:44.344475+08:00","created_by":"David Ten","updated_at":"2026-02-17T06:27:56.12475+08:00","closed_at":"2026-02-17T06:27:56.12475+08:00","close_reason":"All 17 packages created, boundary test passes (7/7), bug in AST checker fixed during review"}
{"id":"StockScreenClaude-7yl","title":"B3: Refactor orchestrator to depend on ports (StockDataProvider, screener registry)","description":"## What\nRefactor ScanOrchestrator so it depends on abstract ports (Protocols) instead of directly importing IO services. The orchestrator becomes a domain service that can be unit-tested with fake providers.\n\n## Current State (backend/app/scanners/scan_orchestrator.py, ~501 lines)\n- Constructor imports and instantiates DataPreparationLayer directly\n- Calls data prep methods that hit Redis cache + yfinance\n- Mixes pure scoring (composite/rating) with IO coordination (data fetch)\n\n## Target State\nOrchestrator constructor accepts:\n- stock_data_provider: StockDataProvider (port/Protocol)\n- screener_registry: dict[ScreenerName, BaseStockScreener]\n- composite_scorer: CompositeScorer (from B1)\n- rating_calculator: RatingCalculator (from B1)\n\n## Files to Modify\n- backend/app/scanners/scan_orchestrator.py — Accept ports via constructor injection\n\n## Files to Create/Update\n- backend/app/domain/scanning/ports.py — Add StockDataProvider Protocol:\n  - prepare_bulk(symbols, requirements, as_of_date=None) -\u003e dict[str, StockData]\n- backend/app/infra/providers/market_data_yfinance.py — Adapter wrapping DataPreparationLayer implementing StockDataProvider\n\n## Key Design Decision\nThe orchestrator itself can live in domain/ (if it only uses ports) or use_cases/ (if it needs session management). Recommend keeping it in domain/ as a domain service, with the use case handling session/transaction boundaries.\n\n## Acceptance Criteria\n- Orchestrator has no direct imports of cache/yfinance/Redis\n- Unit test with fake StockDataProvider produces correct ScanResultItemDomain list\n- Existing scan flow still works (wiring provides real implementations)\n- Boundary test passes\n\n## Depends On\n- B2 (domain types must exist)\n- B1 implicitly (composite/rating extracted)","status":"open","priority":1,"issue_type":"feature","owner":"davidten7@gmail.com","created_at":"2026-02-17T00:00:35.424448+08:00","created_by":"David Ten","updated_at":"2026-02-17T00:00:35.424448+08:00","dependencies":[{"issue_id":"StockScreenClaude-7yl","depends_on_id":"StockScreenClaude-74o","type":"blocks","created_at":"2026-02-17T00:08:09.675755+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-8eu","title":"E2: Create Feature Store domain types — FeatureRunDomain, PublishPolicy, DQ","description":"## What\nCreate domain-layer types for the Feature Store that represent run lifecycle, publish policies, and data quality concepts.\n\n## Files to Create\n- backend/app/domain/feature_store/models.py containing:\n\n  class RunStatus(str, Enum): running, completed, failed, quarantined, published\n  class RunType(str, Enum): daily_snapshot, backfill, manual\n\n  @dataclass FeatureRunDomain:\n    id: int | None\n    as_of_date: date\n    run_type: RunType\n    status: RunStatus\n    correlation_id: str\n    code_version: str\n    universe_hash: str\n    input_hash: str\n    stats: RunStats | None\n    warnings: list[str]\n\n  @dataclass RunStats:\n    total_symbols: int\n    processed_symbols: int\n    failed_symbols: int\n    duration_seconds: float\n\n  @dataclass SnapshotRef:\n    run_id: int\n    as_of_date: date\n    status: RunStatus\n\n- backend/app/domain/feature_store/quality.py — Pure DQ check functions:\n  - check_row_count(expected, actual, threshold=0.9) -\u003e DQResult\n  - check_null_rate(column_nulls, total, max_rate=0.05) -\u003e DQResult\n  - check_score_distribution(scores, expected_mean_range) -\u003e DQResult\n  - is_publishable(dq_results: list[DQResult]) -\u003e bool\n\n  @dataclass DQResult:\n    check_name: str\n    passed: bool\n    actual_value: float\n    threshold: float\n    message: str\n\n## Why\nDQ checks as pure domain functions means they can be unit-tested trivially and reused across different run types (daily, backfill, manual).\n\n## Acceptance Criteria\n- All types are pure dataclasses with no IO imports\n- DQ functions are pure (input → output, no side effects)\n- Boundary test passes (domain-only imports)\n\n## Depends On\n- E1 (schema context needed, though types are independent)","status":"open","priority":2,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-17T00:03:48.530569+08:00","created_by":"David Ten","updated_at":"2026-02-17T00:03:48.530569+08:00","dependencies":[{"issue_id":"StockScreenClaude-8eu","depends_on_id":"StockScreenClaude-9m7","type":"blocks","created_at":"2026-02-17T00:08:09.671305+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-8ms","title":"Unify rate limiting into Redis-backed distributed limiter","status":"closed","priority":1,"issue_type":"feature","owner":"davidten7@gmail.com","created_at":"2026-02-16T11:59:13.749947+08:00","created_by":"David Ten","updated_at":"2026-02-16T12:05:36.478392+08:00","closed_at":"2026-02-16T12:05:36.478392+08:00","close_reason":"Implemented Redis-backed distributed rate limiter with Lua script, fallback, recovery, and timeout. 13 files modified (1 new, 12 updated). 11 unit tests pass."}
{"id":"StockScreenClaude-9cs","title":"Universe Unification — Single typed concept across API, DB, and business logic","status":"closed","priority":1,"issue_type":"feature","owner":"davidten7@gmail.com","created_at":"2026-02-16T14:48:26.272075+08:00","created_by":"David Ten","updated_at":"2026-02-16T14:58:30.138928+08:00","closed_at":"2026-02-16T14:58:30.138928+08:00","close_reason":"All 17 tasks implemented, 68 tests passing"}
{"id":"StockScreenClaude-9in","title":"T16: End-to-end verification of all universe types","status":"closed","priority":2,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-16T14:49:41.861706+08:00","created_by":"David Ten","updated_at":"2026-02-16T14:58:29.82632+08:00","closed_at":"2026-02-16T14:58:29.82632+08:00","close_reason":"E2E verification - requires running server, manual testing needed","dependencies":[{"issue_id":"StockScreenClaude-9in","depends_on_id":"StockScreenClaude-bkd","type":"blocks","created_at":"2026-02-16T14:50:47.100837+08:00","created_by":"David Ten"},{"issue_id":"StockScreenClaude-9in","depends_on_id":"StockScreenClaude-5pe","type":"blocks","created_at":"2026-02-16T14:50:47.384978+08:00","created_by":"David Ten"},{"issue_id":"StockScreenClaude-9in","depends_on_id":"StockScreenClaude-9zn","type":"blocks","created_at":"2026-02-16T14:50:47.683049+08:00","created_by":"David Ten"},{"issue_id":"StockScreenClaude-9in","depends_on_id":"StockScreenClaude-17g","type":"blocks","created_at":"2026-02-16T14:50:47.984589+08:00","created_by":"David Ten"},{"issue_id":"StockScreenClaude-9in","depends_on_id":"StockScreenClaude-3wq","type":"blocks","created_at":"2026-02-16T14:50:48.288328+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-9m7","title":"E1: Create Feature Store SQLAlchemy models + migration","description":"## What\nCreate the database schema for the Feature Store — the core persistence layer for pre-computed screening results.\n\n## Tables to Create (see v3 plan Appendix A)\n\n### feature_runs\n- id (Integer PK, autoincrement)\n- as_of_date (Date, indexed)\n- run_type (Text) — daily_snapshot / backfill / manual\n- status (Text) — running / completed / failed / quarantined / published\n- created_at, completed_at, published_at (DateTime)\n- code_version (Text) — git SHA or app version\n- universe_hash (Text) — hash of resolved symbol list for reproducibility\n- input_hash (Text) — hash of config/criteria for cache-busting\n- correlation_id (Text, indexed) — traces across API→Celery→DB\n- stats_json (JSON) — row counts, timing, coverage stats\n- warnings_json (JSON) — DQ warnings, partial failures\n\n### feature_run_universe_symbols\n- run_id (FK → feature_runs.id, indexed)\n- symbol (Text, indexed)\n- PRIMARY KEY (run_id, symbol)\n\n### stock_feature_daily\n- run_id (FK → feature_runs.id, indexed)\n- symbol (Text, indexed)\n- as_of_date (Date, indexed) — denormalized for query convenience\n- composite_score (Real, indexed)\n- overall_rating (Integer, indexed)\n- passes_count (Integer)\n- details_json (JSON) — per-screener breakdown\n- PRIMARY KEY (run_id, symbol)\n\n### feature_run_pointers\n- key (Text PK) — e.g. 'latest_published'\n- run_id (FK → feature_runs.id)\n- updated_at (DateTime)\n\n## Indexes (minimum for query-per-request)\n- stock_feature_daily(run_id, composite_score DESC)\n- stock_feature_daily(run_id, overall_rating DESC)\n- stock_feature_daily(run_id, symbol)\n\n## Files to Create\n- backend/app/infra/db/models/feature_store.py — SQLAlchemy model classes\n- backend/migrations/add_feature_store_tables.py — Migration script\n\n## Acceptance Criteria\n- All tables created successfully on fresh DB\n- Migration runs cleanly on existing production DB\n- Foreign key constraints enforced\n- Indexes verified with EXPLAIN on common queries\n\n## Depends On\n- A2 (infra/db structure must exist)","status":"open","priority":2,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-17T00:03:37.471434+08:00","created_by":"David Ten","updated_at":"2026-02-17T00:03:37.471434+08:00","dependencies":[{"issue_id":"StockScreenClaude-9m7","depends_on_id":"StockScreenClaude-7lw","type":"blocks","created_at":"2026-02-17T00:08:11.214284+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-9zn","title":"T8: Wire migration into app startup and remove destructive cleanup","status":"closed","priority":1,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-16T14:49:13.421196+08:00","created_by":"David Ten","updated_at":"2026-02-16T14:58:23.90261+08:00","closed_at":"2026-02-16T14:58:23.90261+08:00","close_reason":"Implemented in universe unification session","dependencies":[{"issue_id":"StockScreenClaude-9zn","depends_on_id":"StockScreenClaude-26z","type":"blocks","created_at":"2026-02-16T14:50:15.528174+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-a4x","title":"Add Stuck Task Detection and Force Cancel","description":"Add heartbeat tracking to DataFetchLock with update_heartbeat(), get_minutes_since_heartbeat(), and force_release(). Add POST /v1/cache/force-cancel endpoint.","status":"closed","priority":2,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-05T15:01:10.404607+08:00","created_by":"David Ten","updated_at":"2026-02-05T15:09:27.955686+08:00","closed_at":"2026-02-05T15:09:27.955686+08:00","close_reason":"Added get_current_task() method with heartbeat data to DataFetchLock"}
{"id":"StockScreenClaude-aam","title":"EPIC-E: Feature Store Schema — Models, Domain Types, Repositories","description":"Implement the Feature Store database schema and repository layer. Includes SQLAlchemy models for FeatureRun, StockFeatureDaily, FeatureRunUniverseSymbols, and FeatureRunPointers. Also includes domain types (FeatureRunDomain, PublishPolicy, DQ types) and repository implementations with atomic publish semantics. Corresponds to Phase E of the v3 architecture plan + Appendix A.","status":"open","priority":2,"issue_type":"epic","owner":"davidten7@gmail.com","created_at":"2026-02-16T23:58:56.050999+08:00","created_by":"David Ten","updated_at":"2026-02-16T23:58:56.050999+08:00"}
{"id":"StockScreenClaude-aef","title":"F3: Implement DQ checks + quarantine semantics for feature runs","description":"## What\nImplement the data quality validation pipeline that gates publishing of feature runs. Bad data should be quarantined, not published.\n\n## Components\n\n### Already Created (E2)\n- domain/feature_store/quality.py — Pure DQ check functions\n\n### To Create\n- backend/app/use_cases/feature_store/publish_run.py — PublishFeatureRunUseCase\n  - Input: run_id, dq_overrides (optional, for manual override)\n  - Logic:\n    1. Load run from FeatureRunRepository\n    2. Load snapshot stats (row count, null rates, score distribution)\n    3. Run DQ checks (domain functions)\n    4. If all pass → publish_atomically(run_id)\n    5. If any fail → mark_quarantined(run_id, dq_report)\n    6. Return publish result with DQ report\n\n### DQ Checks to Implement\n1. Row count check: actual rows \u003e= 90% of universe size\n2. Null rate check: composite_score null rate \u003c 5%\n3. Score distribution: mean composite score within expected range\n4. Rating distribution: at least some stocks in each rating bucket (sanity)\n5. Symbol coverage: check no unexpected symbols missing vs universe\n\n## Quarantine Behavior\n- Quarantined runs are NOT published (latest pointer unchanged)\n- Quarantined runs are kept in DB for inspection/debugging\n- Admin can manually publish a quarantined run via override flag\n- DQ report stored in warnings_json for visibility\n\n## Acceptance Criteria\n- DQ checks run automatically during snapshot build (F1 calls this)\n- Quarantined runs visible but not served to API consumers\n- Manual publish override works for admin use cases\n- Unit tests cover each DQ check independently\n\n## Depends On\n- F1 (called during snapshot build)","status":"open","priority":2,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-17T00:04:30.088405+08:00","created_by":"David Ten","updated_at":"2026-02-17T00:04:30.088405+08:00","dependencies":[{"issue_id":"StockScreenClaude-aef","depends_on_id":"StockScreenClaude-0ch","type":"blocks","created_at":"2026-02-17T00:08:11.470855+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-ban","title":"A3: Refactor POST /scans to use CreateScanUseCase","description":"## What\nRefactor the POST /api/v1/scans endpoint to use CreateScanUseCase, proving the layered architecture works end-to-end.\n\n## Current State (backend/app/api/v1/scans.py)\nThe create_scan() router function (~100 lines) currently:\n1. Parses request body (universe definition, screener config)\n2. Resolves universe symbols via universe_resolver\n3. Creates Scan DB record directly\n4. Dispatches Celery task\nAll in one function — mixing transport, business logic, and IO.\n\n## Target State\nRouter create_scan() becomes:\n1. Parse/validate request (FastAPI layer)\n2. Call CreateScanUseCase.execute(config) via Depends()\n3. Return response\n\n## Files to Create\n- backend/app/use_cases/scanning/create_scan.py — CreateScanUseCase class\n- backend/app/domain/scanning/ports.py — ScanRepository, UniverseRepository protocols (initial)\n\n## Files to Modify\n- backend/app/api/v1/scans.py — Slim down create_scan to call use case\n\n## Additional Requirements\n- Add idempotency_key support: repeated POST with same key returns existing scan\n- Celery task dispatch only after DB commit (use on_commit hook or outbox pattern)\n- CreateScanUseCase should be unit-testable with fake repos\n\n## Acceptance Criteria\n- Same API input/output (no breaking changes)\n- Router function is \u003c20 lines\n- Unit test for CreateScanUseCase with fake repos\n- Idempotent: same idempotency_key returns same scan_id\n\n## Depends On\n- A2 (wiring/bootstrap must exist)","status":"closed","priority":1,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-17T00:00:02.527129+08:00","created_by":"David Ten","updated_at":"2026-02-17T06:49:09.381705+08:00","closed_at":"2026-02-17T06:49:09.381705+08:00","close_reason":"Refactored POST /scans to use CreateScanUseCase. Router is ~20 lines, use case is unit-testable with fake repos, idempotency_key support added, all 31 tests pass, layer boundaries enforced.","dependencies":[{"issue_id":"StockScreenClaude-ban","depends_on_id":"StockScreenClaude-7lw","type":"blocks","created_at":"2026-02-17T00:08:10.269652+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-bkd","title":"T10: Update create_scan() to use resolver and persist structured fields","status":"closed","priority":1,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-16T14:49:19.474992+08:00","created_by":"David Ten","updated_at":"2026-02-16T14:58:23.998429+08:00","closed_at":"2026-02-16T14:58:23.998429+08:00","close_reason":"Implemented in universe unification session","dependencies":[{"issue_id":"StockScreenClaude-bkd","depends_on_id":"StockScreenClaude-z91","type":"blocks","created_at":"2026-02-16T14:50:22.96403+08:00","created_by":"David Ten"},{"issue_id":"StockScreenClaude-bkd","depends_on_id":"StockScreenClaude-47f","type":"blocks","created_at":"2026-02-16T14:50:23.252435+08:00","created_by":"David Ten"},{"issue_id":"StockScreenClaude-bkd","depends_on_id":"StockScreenClaude-cne","type":"blocks","created_at":"2026-02-16T14:50:23.541123+08:00","created_by":"David Ten"},{"issue_id":"StockScreenClaude-bkd","depends_on_id":"StockScreenClaude-eh1","type":"blocks","created_at":"2026-02-16T14:50:23.949639+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-c6b","title":"Update Frontend API Client","description":"Add getCacheHealth(), refreshCache(mode), and forceCancelRefresh() to frontend/src/api/cache.js.","status":"closed","priority":2,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-05T15:01:11.620647+08:00","created_by":"David Ten","updated_at":"2026-02-05T15:10:36.427839+08:00","closed_at":"2026-02-05T15:10:36.427839+08:00","close_reason":"Added getCacheHealth(), refreshCache(mode), and forceCancelRefresh() functions"}
{"id":"StockScreenClaude-cne","title":"T6: Add get_universe_definition() reconstruction helper to Scan model","status":"closed","priority":2,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-16T14:49:06.712122+08:00","created_by":"David Ten","updated_at":"2026-02-16T14:58:23.815815+08:00","closed_at":"2026-02-16T14:58:23.815815+08:00","close_reason":"Implemented in universe unification session","dependencies":[{"issue_id":"StockScreenClaude-cne","depends_on_id":"StockScreenClaude-47f","type":"blocks","created_at":"2026-02-16T14:50:08.217163+08:00","created_by":"David Ten"},{"issue_id":"StockScreenClaude-cne","depends_on_id":"StockScreenClaude-eeh","type":"blocks","created_at":"2026-02-16T14:50:08.513437+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-dls","title":"T6: Query performance guardrails — prevent N+1 regressions","description":"## What\nAdd lightweight performance benchmarks that catch query regressions (especially N+1 queries) in the feature store path.\n\n## Files to Create\n- backend/tests/performance/test_query_performance.py\n\n## Test Design\n1. Seed a minimal feature store table (~500 rows, 1 run)\n2. Run common query patterns:\n   - Paginated results (page 1 of 20)\n   - Filtered by rating \u003e= 3\n   - Sorted by composite_score DESC\n   - Filter + sort combined\n3. Assert for each query:\n   - Query count \u003c= expected (usually 2: count + select)\n   - Execution time \u003c budget (e.g., 100ms for SQLite)\n4. Compare against legacy scan_results queries for same patterns\n\n## Implementation\n- Use SQLAlchemy event listeners to count queries\n- Use pytest fixtures to seed consistent test data\n- Budgets should be generous (not flaky) but catch order-of-magnitude regressions\n\n## Acceptance Criteria\n- No N+1 queries in any tested pattern\n- Query count assertions prevent regression\n- Timing budgets catch major performance issues\n- Tests run in \u003c10 seconds (including DB setup)\n\n## Depends On\n- G2 (feature store query path must exist)","status":"open","priority":3,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-17T00:06:16.26408+08:00","created_by":"David Ten","updated_at":"2026-02-17T00:06:16.26408+08:00","dependencies":[{"issue_id":"StockScreenClaude-dls","depends_on_id":"StockScreenClaude-78r","type":"blocks","created_at":"2026-02-17T00:08:13.207028+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-dwz","title":"Fix cache warming deadlock risk","status":"closed","priority":1,"issue_type":"bug","owner":"davidten7@gmail.com","created_at":"2026-02-16T10:28:19.708173+08:00","created_by":"David Ten","updated_at":"2026-02-16T10:33:06.228819+08:00","closed_at":"2026-02-16T10:33:06.228819+08:00","close_reason":"Implemented all 3 changes: atomic Lua release/extend, re-entrant acquire, extracted _impl functions"}
{"id":"StockScreenClaude-e26","title":"EPIC-D: Read APIs — Six Use Cases for Scan Query Endpoints","description":"Refactor all scan read/query endpoints from direct DB access in routers to proper use cases. Covers: GetScanResults, GetFilterOptions, GetSingleResult, GetPeers, ExportScanResults, and the new ExplainStock use case. Routers become thin transport wrappers. Corresponds to Phase D of the v3 architecture plan.","status":"open","priority":2,"issue_type":"epic","owner":"davidten7@gmail.com","created_at":"2026-02-16T23:58:57.634109+08:00","created_by":"David Ten","updated_at":"2026-02-16T23:58:57.634109+08:00"}
{"id":"StockScreenClaude-e2m","title":"F2: Add Celery beat scheduled task for daily snapshot","description":"## What\nCreate a thin Celery task wrapper for BuildDailyFeatureSnapshotUseCase and register it in Celery beat schedule.\n\n## Files to Create\n- backend/app/interfaces/tasks/feature_store_tasks.py — Thin Celery task:\n  1. Open UoW\n  2. Wire use case via bootstrap\n  3. Call BuildDailyFeatureSnapshotUseCase.execute(as_of_date=None for 'today')\n  4. Return summary stats\n\n## Files to Modify\n- backend/app/celery_app.py (or beat schedule config) — Add beat entry:\n  - Schedule: daily at configured time (e.g., 6:00 AM ET on weekdays)\n  - Queue: data_fetch (respects rate limits via single-worker queue)\n  - Task: feature_store_tasks.build_daily_snapshot\n\n## Considerations\n- Must route to data_fetch queue since it makes upstream API calls\n- Should be idempotent: if run twice for same date, creates separate runs\n- Add monitoring: log correlation_id at task start for traceability\n\n## Acceptance Criteria\n- Beat schedule includes daily snapshot task\n- Task is thin wrapper (\u003c30 lines)\n- Runs on data_fetch queue\n- Logs correlation_id for tracing\n\n## Depends On\n- F1 (BuildDailyFeatureSnapshotUseCase must exist)","status":"open","priority":2,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-17T00:04:20.010277+08:00","created_by":"David Ten","updated_at":"2026-02-17T00:04:20.010277+08:00","dependencies":[{"issue_id":"StockScreenClaude-e2m","depends_on_id":"StockScreenClaude-0ch","type":"blocks","created_at":"2026-02-17T00:08:11.262043+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-eeh","title":"T1: Create universe schema module with enums and UniverseDefinition model","status":"closed","priority":1,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-16T14:48:50.675642+08:00","created_by":"David Ten","updated_at":"2026-02-16T14:58:23.582175+08:00","closed_at":"2026-02-16T14:58:23.582175+08:00","close_reason":"Implemented in universe unification session"}
{"id":"StockScreenClaude-eh1","title":"T9: Update ScanCreateRequest to accept structured universe","status":"closed","priority":1,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-16T14:49:16.333681+08:00","created_by":"David Ten","updated_at":"2026-02-16T14:58:23.949454+08:00","closed_at":"2026-02-16T14:58:23.949454+08:00","close_reason":"Implemented in universe unification session","dependencies":[{"issue_id":"StockScreenClaude-eh1","depends_on_id":"StockScreenClaude-eeh","type":"blocks","created_at":"2026-02-16T14:50:19.13969+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-fni","title":"T2: Write unit tests for UniverseDefinition","status":"closed","priority":2,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-16T14:48:54.276324+08:00","created_by":"David Ten","updated_at":"2026-02-16T14:58:23.627957+08:00","closed_at":"2026-02-16T14:58:23.627957+08:00","close_reason":"Implemented in universe unification session","dependencies":[{"issue_id":"StockScreenClaude-fni","depends_on_id":"StockScreenClaude-eeh","type":"blocks","created_at":"2026-02-16T14:49:53.90838+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-hkw","title":"EPIC-F: Daily Snapshot — Build Use Case, Celery Beat, DQ Checks","description":"Implement the offline daily snapshot pipeline. BuildDailyFeatureSnapshotUseCase resolves the trading day, loads universe, bulk-prepares data, runs multi-screener orchestration, persists to feature store, runs DQ checks, and publishes atomically (or quarantines on DQ failure). Adds Celery beat scheduling. Corresponds to Phase F of the v3 architecture plan.","status":"open","priority":2,"issue_type":"epic","owner":"davidten7@gmail.com","created_at":"2026-02-16T23:59:00.008903+08:00","created_by":"David Ten","updated_at":"2026-02-16T23:59:00.008903+08:00"}
{"id":"StockScreenClaude-hq1","title":"T13: Update frontend API docs in scans.js","status":"closed","priority":2,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-16T14:49:31.685869+08:00","created_by":"David Ten","updated_at":"2026-02-16T14:58:24.129356+08:00","closed_at":"2026-02-16T14:58:24.129356+08:00","close_reason":"Implemented in universe unification session","dependencies":[{"issue_id":"StockScreenClaude-hq1","depends_on_id":"StockScreenClaude-bkd","type":"blocks","created_at":"2026-02-16T14:50:36.127498+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-iai","title":"D1: Create GetScanResultsUseCase — paginated, filtered, sorted query","description":"## What\nExtract the scan results query logic from the router into a proper use case. This is the most complex read endpoint — it handles filtering, sorting, and pagination.\n\n## Current State (backend/app/api/v1/scans.py)\nThe get_scan_results endpoint (~350 lines of filter construction) directly:\n- Parses 20+ filter parameters from query string\n- Builds SQLAlchemy queries with JSON extraction for details fields\n- Handles sorting on both regular columns and JSON-extracted fields\n- Implements pagination\n- Maps DB rows to response format\n\n## Files to Create\n- backend/app/use_cases/scanning/get_scan_results.py — GetScanResultsUseCase\n  - Input: scan_id, FilterSpec, SortSpec, PagingSpec (domain types from B2)\n  - Output: Page[ScanResultItemDomain] with total count\n  - Uses ScanResultRepository.query_results() port\n\n- backend/app/infra/query/scan_query_sqlalchemy.py — SQLAlchemy query builder\n  - Translates FilterSpec → SQLAlchemy WHERE clauses\n  - Translates SortSpec → ORDER BY\n  - Handles JSON field extraction for details-based filters\n  - Implements efficient COUNT + SELECT in single query\n\n## Why This Architecture\nBy putting the query builder in infra/ and the use case in use_cases/, we can:\n- Test the use case with a fake repo (returns canned data)\n- Test the query builder against a real SQLite DB in integration tests\n- Swap the query builder for a Postgres-specific one later without touching use case\n\n## Acceptance Criteria\n- Use case returns same results as current endpoint for all filter combinations\n- Router becomes thin: parse params → build specs → call use case → return response\n- FilterSpec supports all current filter fields\n- Pagination returns correct total counts\n- No N+1 queries\n\n## Depends On\n- A2 (wiring/bootstrap for injection)","status":"open","priority":2,"issue_type":"feature","owner":"davidten7@gmail.com","created_at":"2026-02-17T00:02:38.319243+08:00","created_by":"David Ten","updated_at":"2026-02-17T00:02:38.319243+08:00","dependencies":[{"issue_id":"StockScreenClaude-iai","depends_on_id":"StockScreenClaude-7lw","type":"blocks","created_at":"2026-02-17T00:08:10.941278+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-ile","title":"G1: Add Scan.feature_run_id binding","description":"## What\nAdd a feature_run_id column to the Scan model so scans can be bound to a specific Feature Store run. This is the bridge between the online scan API and the offline feature store.\n\n## Files to Modify\n- backend/app/models/scan.py (or wherever Scan model lives) — Add:\n  - feature_run_id (Integer, FK → feature_runs.id, nullable=True)\n  - Nullable because legacy scans predate the feature store\n\n- backend/app/use_cases/scanning/create_scan.py — Update CreateScanUseCase:\n  - On scan creation, look up latest published feature run\n  - If available, bind scan.feature_run_id = latest_run.id\n  - If no published run exists, leave null (legacy behavior)\n\n## Migration\n- backend/migrations/add_scan_feature_run_id.py — Add column with ALTER TABLE\n\n## Why This Design\nBinding at creation time (not query time) ensures:\n1. Scan results are reproducible — they always reference the same snapshot\n2. No 'drift' if a new snapshot publishes while user is viewing results\n3. Clear audit trail of which data backed each scan\n\n## Acceptance Criteria\n- New scans get feature_run_id when a published run exists\n- Legacy scans (null feature_run_id) continue working unchanged\n- Migration runs cleanly on existing DB\n- CreateScanUseCase updated with binding logic\n\n## Depends On\n- E3 (Feature Store repos must exist for lookup)\n- A3 (CreateScanUseCase must exist)","status":"open","priority":2,"issue_type":"feature","owner":"davidten7@gmail.com","created_at":"2026-02-17T00:04:36.509042+08:00","created_by":"David Ten","updated_at":"2026-02-17T00:04:36.509042+08:00","dependencies":[{"issue_id":"StockScreenClaude-ile","depends_on_id":"StockScreenClaude-rwf","type":"blocks","created_at":"2026-02-17T00:08:09.679096+08:00","created_by":"David Ten"},{"issue_id":"StockScreenClaude-ile","depends_on_id":"StockScreenClaude-ban","type":"blocks","created_at":"2026-02-17T00:08:10.05618+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-jhs","title":"Z1: Remove legacy compute-per-request scan path","description":"## What\nOnce the feature store is stable and all endpoints are migrated, remove the legacy compute-per-request code path or downgrade it to an opt-in debug mode.\n\n## What to Remove/Deprecate\n- Legacy scan_results table writes (new scans use feature store)\n- Direct DB queries from routers (now in use cases)\n- Inline filter construction in scan router (~350 lines)\n- Business logic in Celery tasks (now in use cases)\n- Old ScanOrchestrator methods that were replaced by domain functions\n\n## What to Keep\n- scan_results table itself (historical data)\n- Backward-compatible API responses\n- Legacy path as opt-in debug mode (env var flag) for comparison testing\n\n## Migration Safety\n- Feature flag: SCAN_LEGACY_MODE=true to re-enable old path if issues found\n- Monitor for 2 weeks before removing feature flag\n- Keep scan_results table data for historical queries\n\n## Files to Modify\n- backend/app/api/v1/scans.py — Remove legacy code paths\n- backend/app/tasks/scan_tasks.py — Remove business logic (should already be thin from C2)\n- backend/app/scanners/scan_orchestrator.py — Remove replaced methods\n\n## Acceptance Criteria\n- All endpoints serve from feature store by default\n- Legacy mode available via feature flag\n- No dead code remaining (except behind feature flag)\n- All tests pass\n\n## Depends On\n- G3 (all ancillary endpoints migrated)\n- G4 (native endpoints working)","status":"open","priority":3,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-17T00:06:29.157319+08:00","created_by":"David Ten","updated_at":"2026-02-17T00:06:29.157319+08:00","dependencies":[{"issue_id":"StockScreenClaude-jhs","depends_on_id":"StockScreenClaude-t5g","type":"blocks","created_at":"2026-02-17T00:08:13.500466+08:00","created_by":"David Ten"},{"issue_id":"StockScreenClaude-jhs","depends_on_id":"StockScreenClaude-6dx","type":"blocks","created_at":"2026-02-17T00:08:13.779209+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-k92","title":"Add Smart Refresh API Endpoint","description":"Add POST /v1/cache/refresh endpoint that triggers the smart_refresh_cache task. Handles already_running detection and returns task_id.","status":"closed","priority":1,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-05T15:01:08.497602+08:00","created_by":"David Ten","updated_at":"2026-02-05T15:10:12.164074+08:00","closed_at":"2026-02-05T15:10:12.164074+08:00","close_reason":"Added GET /v1/cache/health, POST /v1/cache/refresh, and POST /v1/cache/force-cancel endpoints","dependencies":[{"issue_id":"StockScreenClaude-k92","depends_on_id":"StockScreenClaude-szs","type":"blocks","created_at":"2026-02-05T15:02:51.069891+08:00","created_by":"David Ten"},{"issue_id":"StockScreenClaude-k92","depends_on_id":"StockScreenClaude-5rl","type":"blocks","created_at":"2026-02-05T15:02:51.275019+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-mzk","title":"D4: Create GetPeersUseCase","description":"## What\nExtract peer stock lookup into a use case.\n\n## Current State\nThe peers endpoint in scans.py finds stocks in the same sector/industry as a given symbol within the same scan results.\n\n## Files to Create\n- backend/app/use_cases/scanning/get_peers.py — GetPeersUseCase\n  - Input: scan_id, symbol, peer_type (sector/industry)\n  - Output: list[ScanResultItemDomain]\n  - Uses ScanResultRepository port\n\n## Acceptance Criteria\n- Returns same peer list as current endpoint\n- Router is thin wrapper\n- Handles missing symbol gracefully\n\n## Depends On\n- D1 (establishes repository patterns)","status":"open","priority":2,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-17T00:02:51.69309+08:00","created_by":"David Ten","updated_at":"2026-02-17T00:02:51.69309+08:00","dependencies":[{"issue_id":"StockScreenClaude-mzk","depends_on_id":"StockScreenClaude-iai","type":"blocks","created_at":"2026-02-17T00:08:10.506982+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-nrv","title":"EPIC-A: Scaffolding — Package Structure, Wiring, First Use Case","description":"Create the layered architecture package structure (domain/, use_cases/, infra/, interfaces/, wiring/), add dependency injection bootstrap, and refactor the first endpoint (POST /scans → CreateScanUseCase) as proof of concept. This epic establishes the foundation that all subsequent refactoring depends on. Corresponds to Phase A of the v3 architecture plan.","status":"open","priority":1,"issue_type":"epic","owner":"davidten7@gmail.com","created_at":"2026-02-16T23:58:14.608331+08:00","created_by":"David Ten","updated_at":"2026-02-16T23:58:14.608331+08:00"}
{"id":"StockScreenClaude-nu6","title":"T1: Domain unit tests — composite scorer + rating calculator","description":"## What\nWrite comprehensive unit tests for the extracted domain scoring logic. These should be fast (no IO) and cover all edge cases.\n\n## Files to Create\n- backend/tests/unit/domain/test_composite_scorer.py\n- backend/tests/unit/domain/test_rating_calculator.py\n\n## Test Cases for CompositeScorer\n- weighted_average: equal weights, unequal weights, single screener, zero weight\n- maximum: picks highest score correctly\n- minimum: picks lowest score correctly\n- Edge cases: empty screener list, all zeros, all 100s\n- Verify identical output to current ScanOrchestrator._calculate_composite_score\n\n## Test Cases for RatingCalculator\n- Each rating threshold boundary (just above, exactly at, just below)\n- Pass rate impact on rating\n- Edge cases: score of 0, score of 100, no passes\n- Verify identical output to current ScanOrchestrator._calculate_overall_rating\n\n## Parity Verification\nInclude a test that compares domain functions against current orchestrator for a set of known inputs, ensuring the extraction preserved behavior exactly.\n\n## Acceptance Criteria\n- 100% branch coverage on composite.py and rating.py\n- All tests pass in \u003c1 second (no IO)\n- Parity with current orchestrator verified\n\n## Depends On\n- B1 (domain scoring must be extracted)","status":"open","priority":2,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-17T00:05:35.8701+08:00","created_by":"David Ten","updated_at":"2026-02-17T00:05:35.8701+08:00","dependencies":[{"issue_id":"StockScreenClaude-nu6","depends_on_id":"StockScreenClaude-3z0","type":"blocks","created_at":"2026-02-17T00:08:11.78513+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-rwf","title":"E3: Create Feature Store repositories — FeatureRunRepo, FeatureStoreRepo","description":"## What\nImplement the repository layer for Feature Store with atomic publish semantics.\n\n## Files to Create\n\n### Domain ports (protocols)\n- backend/app/domain/feature_store/ports.py:\n  - FeatureRunRepository Protocol:\n    - start_run(as_of_date, run_type, code_version, universe_hash, input_hash, correlation_id) -\u003e FeatureRunDomain\n    - mark_completed(run_id, stats, warnings)\n    - mark_quarantined(run_id, dq_report)\n    - publish_atomically(run_id) — MUST update status + pointer in same transaction\n    - get_latest_published() -\u003e FeatureRunDomain | None\n    - get_run(run_id) -\u003e FeatureRunDomain\n\n  - FeatureStoreRepository Protocol:\n    - upsert_snapshot_rows(run_id, rows: list[FeatureRowWrite])\n    - save_run_universe_symbols(run_id, symbols: list[str])\n    - query_latest(spec: FilterSpec, sort: SortSpec, paging: PagingSpec) -\u003e Page[FeatureRow]\n    - query_run(run_id, spec, sort, paging) -\u003e Page[FeatureRow]\n\n### Infra implementations\n- backend/app/infra/db/repositories/feature_run_repo_sqlalchemy.py\n- backend/app/infra/db/repositories/feature_store_repo_sqlalchemy.py\n\n## Critical: Atomic Publish\npublish_atomically() must in a SINGLE transaction:\n1. Set feature_runs.status = 'published', published_at = now\n2. Update feature_run_pointers where key='latest_published' → run_id\nThis prevents readers from seeing a mixed state.\n\n## Acceptance Criteria\n- Repositories implement all port methods\n- Atomic publish verified: no partial state observable\n- Integration test with SQLite confirms CRUD operations\n- Query methods support FilterSpec translation (reuse patterns from D1)\n\n## Depends On\n- E2 (domain types must exist)\n- E1 implicitly (DB models must exist)","status":"open","priority":2,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-17T00:03:52.638505+08:00","created_by":"David Ten","updated_at":"2026-02-17T00:03:52.638505+08:00","dependencies":[{"issue_id":"StockScreenClaude-rwf","depends_on_id":"StockScreenClaude-8eu","type":"blocks","created_at":"2026-02-17T00:08:09.992353+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-szs","title":"Implement Cache Health Status Method","description":"Add get_cache_health_status() to PriceCacheService with SPY-based O(1) health check. Implements 6 states: fresh/updating/stuck/partial/stale/error. Includes warmup metadata tracking and early morning edge case handling.","status":"closed","priority":1,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-05T15:01:01.439026+08:00","created_by":"David Ten","updated_at":"2026-02-05T15:08:25.860399+08:00","closed_at":"2026-02-05T15:08:25.860399+08:00","close_reason":"Implemented get_cache_health_status() with 6 states, SPY-based health check, warmup metadata, and heartbeat tracking"}
{"id":"StockScreenClaude-t5g","title":"G3: Migrate ancillary scan endpoints to feature store source","description":"## What\nUpdate filter-options, peers, single-result, and export endpoints to use the same dual-source pattern as GetScanResults (G2).\n\n## Endpoints to Update\n- GetFilterOptionsUseCase (D2) — query feature store for distinct values when scan is bound\n- GetSingleResultUseCase (D3) — read from feature store when scan is bound\n- GetPeersUseCase (D4) — find peers in feature store when scan is bound\n- ExportScanResultsUseCase (D5) — export from feature store when scan is bound\n\n## Pattern\nEach use case already queries via repository ports. The change is:\n1. Check scan.feature_run_id\n2. If bound → use FeatureStoreRepository methods\n3. If unbound → use legacy ScanResultRepository methods\n\n## Files to Modify\n- All D-series use case files\n- May need to add feature-store-specific query methods to FeatureStoreRepository\n\n## Acceptance Criteria\n- All ancillary endpoints work with both bound and unbound scans\n- Same response format regardless of data source\n- No breaking API changes\n- Integration test confirms feature store path returns correct data\n\n## Depends On\n- G2 (dual-source pattern established)","status":"open","priority":2,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-17T00:05:14.903501+08:00","created_by":"David Ten","updated_at":"2026-02-17T00:05:14.903501+08:00","dependencies":[{"issue_id":"StockScreenClaude-t5g","depends_on_id":"StockScreenClaude-78r","type":"blocks","created_at":"2026-02-17T00:08:11.213828+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-toj","title":"D3: Create GetSingleResultUseCase","description":"## What\nExtract single scan result lookup into a use case.\n\n## Current State\nThe single-result endpoint in scans.py fetches one ScanResult by scan_id + symbol, extracts JSON details, and returns enriched data.\n\n## Files to Create\n- backend/app/use_cases/scanning/get_single_result.py — GetSingleResultUseCase\n  - Input: scan_id, symbol\n  - Output: ScanResultItemDomain (full detail)\n  - Uses ScanResultRepository port\n\n## Acceptance Criteria\n- Returns same data as current endpoint\n- Router is thin wrapper\n- Handles 'not found' gracefully (raises domain NotFoundError)\n\n## Depends On\n- D1 (establishes repository patterns)","status":"open","priority":2,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-17T00:02:43.984599+08:00","created_by":"David Ten","updated_at":"2026-02-17T00:02:43.984599+08:00","dependencies":[{"issue_id":"StockScreenClaude-toj","depends_on_id":"StockScreenClaude-iai","type":"blocks","created_at":"2026-02-17T00:08:10.140202+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-vkg","title":"T3: Integration tests — repository implementations with SQLite","description":"## What\nWrite integration tests that verify SQLAlchemy repository implementations against a real SQLite database.\n\n## Files to Create\n- backend/tests/integration/test_scan_repo_sqlalchemy.py\n- backend/tests/integration/test_feature_store_repo_sqlalchemy.py\n- backend/tests/integration/conftest.py — Shared fixtures:\n  - in-memory SQLite engine\n  - create_all tables\n  - session factory\n  - cleanup between tests\n\n## Test Cases for ScanRepository\n- CRUD operations: create, get, update_status\n- Idempotency key: duplicate create returns existing\n- Status transitions: pending → running → completed/failed\n\n## Test Cases for FeatureStoreRepository\n- upsert_snapshot_rows: bulk insert and update\n- query_latest: returns rows from latest published run\n- query_run: returns rows from specific run\n- Atomic publish: verify status + pointer update in one transaction\n- FilterSpec translation: common filter/sort combinations\n\n## Key Verification\n- Atomic publish: simulate crash between status update and pointer update → verify no partial state\n- Query performance: no N+1 queries (check query count)\n\n## Acceptance Criteria\n- All repository methods tested against real SQLite\n- Atomic publish verified\n- Tests clean up after themselves (no cross-test contamination)\n- Tests run in \u003c10 seconds\n\n## Depends On\n- E3 (Feature Store repos must exist)","status":"open","priority":2,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-17T00:05:52.621416+08:00","created_by":"David Ten","updated_at":"2026-02-17T00:05:52.621416+08:00","dependencies":[{"issue_id":"StockScreenClaude-vkg","depends_on_id":"StockScreenClaude-rwf","type":"blocks","created_at":"2026-02-17T00:08:12.373204+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-wms","title":"EPIC-C: Thin Celery — RunBulkScanUseCase + Thin Task Wrapper","description":"Move all scan business logic out of Celery tasks into RunBulkScanUseCase. The Celery task becomes a thin wrapper that opens a UoW, calls the use case, and reports status. After this epic, task files contain zero business rules. Supports cancellation, checkpointing, and idempotent retries. Corresponds to Phase C of the v3 architecture plan.","status":"open","priority":1,"issue_type":"epic","owner":"davidten7@gmail.com","created_at":"2026-02-16T23:58:54.38017+08:00","created_by":"David Ten","updated_at":"2026-02-16T23:58:54.38017+08:00"}
{"id":"StockScreenClaude-xph","title":"EPIC-B: Domain Extraction — Pure Scoring, Domain Types, Port-Based Orchestrator","description":"Extract pure business logic from ScanOrchestrator into the domain layer. This includes composite scoring policies, rating calculation, domain types (ScanConfig, ScreenerOutput), and refactoring the orchestrator to depend on ports (Protocols) rather than concrete IO services. After this epic, all scoring/rating logic is unit-testable without DB or network. Corresponds to Phase B of the v3 architecture plan.","status":"open","priority":1,"issue_type":"epic","owner":"davidten7@gmail.com","created_at":"2026-02-16T23:58:35.625877+08:00","created_by":"David Ten","updated_at":"2026-02-16T23:58:35.625877+08:00"}
{"id":"StockScreenClaude-yfz","title":"T17: Update existing scan tests for universe changes","status":"closed","priority":2,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-16T14:49:45.931233+08:00","created_by":"David Ten","updated_at":"2026-02-16T14:58:24.266684+08:00","closed_at":"2026-02-16T14:58:24.266684+08:00","close_reason":"Implemented in universe unification session","dependencies":[{"issue_id":"StockScreenClaude-yfz","depends_on_id":"StockScreenClaude-bkd","type":"blocks","created_at":"2026-02-16T14:50:51.393973+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-ykj","title":"EPIC-T: Testing — Domain, Use Case, Integration, Parity, Performance Tests","description":"Comprehensive testing strategy for the refactored architecture. Includes domain unit tests (composite/rating), use case tests with fake repos, integration tests with DB, golden scan parity tests (legacy vs feature store), provider contract tests, and query performance guardrails. Corresponds to Section 7 of the v3 architecture plan.","status":"open","priority":2,"issue_type":"epic","owner":"davidten7@gmail.com","created_at":"2026-02-16T23:59:09.260593+08:00","created_by":"David Ten","updated_at":"2026-02-16T23:59:09.260593+08:00"}
{"id":"StockScreenClaude-yt7","title":"D5: Create ExportScanResultsUseCase","description":"## What\nExtract scan results CSV/Excel export into a use case.\n\n## Current State\nThe export endpoint in scans.py queries results, formats them, and returns a downloadable file.\n\n## Files to Create\n- backend/app/use_cases/scanning/export_scan_results.py — ExportScanResultsUseCase\n  - Input: scan_id, FilterSpec, SortSpec, export_format (csv/xlsx)\n  - Output: bytes (file content) + filename\n  - Uses ScanResultRepository port + domain formatting logic\n\n## Acceptance Criteria\n- Produces identical export files as current endpoint\n- Router is thin wrapper (streams response)\n- Supports same filter/sort options as GetScanResults\n\n## Depends On\n- D1 (establishes query and repository patterns)","status":"open","priority":2,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-17T00:02:51.953914+08:00","created_by":"David Ten","updated_at":"2026-02-17T00:02:51.953914+08:00","dependencies":[{"issue_id":"StockScreenClaude-yt7","depends_on_id":"StockScreenClaude-iai","type":"blocks","created_at":"2026-02-17T00:08:10.910396+08:00","created_by":"David Ten"}]}
{"id":"StockScreenClaude-z91","title":"T3: Create UniverseResolver service","status":"closed","priority":1,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-16T14:48:57.456743+08:00","created_by":"David Ten","updated_at":"2026-02-16T14:58:23.671277+08:00","closed_at":"2026-02-16T14:58:23.671277+08:00","close_reason":"Implemented in universe unification session","dependencies":[{"issue_id":"StockScreenClaude-z91","depends_on_id":"StockScreenClaude-eeh","type":"blocks","created_at":"2026-02-16T14:49:56.973957+08:00","created_by":"David Ten"}]}
